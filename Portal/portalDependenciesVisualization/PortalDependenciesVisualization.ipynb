{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portal Dependencies Visualization\n",
    "Created: June 3, 2024\n",
    "By: Tanner Hammond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit these variables\n",
    "## Portal URL, username, and password\n",
    "portal =''\n",
    "GISuser = '' \n",
    "GISpass = ''\n",
    "\n",
    "## Filters\n",
    "filterType = '' #Params: 'Include' - keep only the types listed; 'Exclude' - don't keep any of the types listed; '' - apply no filter\n",
    "itemFilter = [] #Ex: ['Web Map', 'Map Service', 'Feature Service', 'Site Application','Dashboard','Web Mapping Application','Web Experience','Site Page','Form','CSV','Image']\n",
    "\n",
    "##Export\n",
    "generateNodes = False \n",
    "exportType = '' \n",
    "pointName = '' \n",
    "lineName = ''\n",
    "outPath = r''\n",
    "workspace = r'' \n",
    "\n",
    "##List of all items possible to query in Portal (change only if new types are added to ArcGIS Portal)\n",
    "portalItemTypes = ['360 VR Experience','CityEngine Web Scene','Map Area','Pro Map','Web Map','Web Scene','Feature Collection','Feature Collection Template','Feature Service','Geodata Service','Group Layer','Image Service','KML','KML Collection','Map Service','OGCFeatureServer','Oriented Imagery Catalog','Relational Database Connection','3DTilesService','Scene Service','Vector Tile Service','WFS','WMS','WMTS','Geometry Service','Geocoding Service','Geoprocessing Service','Network Analysis Service','Workflow Manager Service','AppBuilder Extension','AppBuilder Widget Package','Code Attachment','Dashboard','Data Pipeline','Deep Learning Studio Project','Esri Classification Schema','Excalibur Imagery Project','Experience Builder Widget','Experience Builder Widget Package','Form','GeoBIM Application','GeoBIM Project','Hub Event','Hub Initiative','Hub Initiative Template','Hub Page','Hub Project','Hub Site Application','Insights Workbook','Insights Workbook Package','Insights Model','Insights Page','Insights Theme','Insights Data Engineering Workbook','Insights Data Engineering Model','Investigation','Knowledge Studio Project','Mission','Mobile Application','Notebook','Notebook Code Snippet Library','Native Application','Native Application Installer','Ortho Mapping Project','Ortho Mapping Template','Solution','StoryMap','Web AppBuilder Widget','Web Experience','Web Experience Template','Web Mapping Application','Workforce Project','Administrative Report','Apache Parquet','CAD Drawing','Color Set','Content Category Set','CSV','Document Link','Earth configuration','Esri Classifier Definition','Export Package','File Geodatabase','GeoJson','GeoPackage','GML','Image','iWork Keynote','iWork Numbers','iWork Pages','Microsoft Excel','Microsoft Powerpoint','Microsoft Word','PDF','Report Template','Service Definition','Shapefile','SQLite Geodatabase','Statistical Data Collection','StoryMap Theme','Style','Symbol Set','Visio Document','ArcPad Package','Compact Tile Package','Explorer Map','Globe Document','Layout','Map Document','Map Package','Map Template','Mobile Basemap Package','Mobile Map Package','Mobile Scene Package','Project Package','Project Template','Published Map','Scene Document','Task File','Tile Package','Vector Tile Package','Explorer Layer','Image Collection','Layer','Layer Package','Pro Report','Scene Package','3DTilesPackage','Desktop Style','ArcGIS Pro Configuration','Deep Learning Package','Geoprocessing Package','Geoprocessing Package (Pro version)','Geoprocessing Sample','Locator Package','Raster function template','Rule Package','Pro Report Template','ArcGIS Pro Add In','Code Sample','Desktop Add In','Desktop Application','Desktop Application Template','Explorer Add In','Survey123 Add In','Workflow Manager Package']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "from arcgis import GIS\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt #might not need\n",
    "import arcpy\n",
    "from arcpy import env\n",
    "import numpy as np\n",
    "\n",
    "#Connect to ArcGIS Portal\n",
    "gis = GIS(portal, GISuser, GISpass)\n",
    "\n",
    "#Set Workspace\n",
    "if workspace == 'Memory' or 'memory':\n",
    "    pass\n",
    "else:\n",
    "    try:\n",
    "        arcpy.env.workspace = workspace\n",
    "    except:\n",
    "        workspace = arcpy.env.workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions\n",
    "## Convert id dependency type to url when is a rest link\n",
    "def restType(dependencyType, dependency):\n",
    "    if dependencyType == 'id' and '/' in dependency:\n",
    "        dependencyType = 'url'\n",
    "    else:\n",
    "        dependencyType\n",
    "    return(dependencyType)\n",
    "\n",
    "## Generate URLs\n",
    "def urlGen(dependencyType, dependency, portal):\n",
    "    if dependencyType == 'id':\n",
    "        url = portal + f'/home/item.html?id=' + dependency\n",
    "    elif dependencyType == 'url':\n",
    "        url = dependency\n",
    "    else:\n",
    "        url = ''\n",
    "    return(url)\n",
    "\n",
    "## Test if links to dependencies are broken. get title and type\n",
    "def testLink(dependency, dependencyType, url):\n",
    "    if url in linkDict:\n",
    "        broken = linkDict[url][0]\n",
    "        itemType = linkDict[url][1]\n",
    "        itemTitle = linkDict[url][2]\n",
    "    elif dependencyType == 'id':\n",
    "        try:\n",
    "\n",
    "            broken = False\n",
    "            itemType = gis.content.search(dependency, outside_org = True)[0].type\n",
    "            itemTitle = gis.content.search(dependency, outside_org = True)[0].title\n",
    "        except:\n",
    "            broken = True\n",
    "            itemType = ''\n",
    "            itemTitle = ''\n",
    "    elif dependencyType == 'url':\n",
    "        try:\n",
    "            url = url + '?f=pjson'\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                response = requests.get(url)\n",
    "                try:\n",
    "                    item_json = response.json()\n",
    "                    itemType = item_json['type']\n",
    "                    itemTitlte = item_json['title']\n",
    "                    broken = False                                   \n",
    "                except: \n",
    "                    broken = False\n",
    "                    itemType = ''\n",
    "                    itemTitle = ''                \n",
    "            else:\n",
    "                broken = True\n",
    "                itemType = ''\n",
    "                itemTitle = ''\n",
    "        except requests.ConnectionError as e:\n",
    "            broken = True\n",
    "            itemType = ''\n",
    "            itemTitle = ''\n",
    "    else:\n",
    "        broken = ''\n",
    "        itemType = ''\n",
    "        itemTitle = ''\n",
    "        #try:\n",
    "            #broken and itemType\n",
    "        #except:\n",
    "            #broken = ''\n",
    "            #itemType = ''\n",
    "    if url not in linkDict:\n",
    "        linkDict[url] = ([broken, itemType, itemTitle])\n",
    "    return pd.Series([broken, itemType, itemTitle])\n",
    "\n",
    "## Combine columns by replacing blank values\n",
    "def combineColumns(check, column1, column2):\n",
    "    if check != '':\n",
    "        value = column1\n",
    "    elif check == '':\n",
    "        value = column2\n",
    "    else:\n",
    "        value = ''\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All Portal Items & Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check filter against item type list\n",
    "if filterType == 'Exclude':\n",
    "    itemFilter[:] = [i for i in portalItemTypes if i not in itemFilter]\n",
    "elif filterType == 'Include':\n",
    "    itemFilter[:] = [i for i in portalItemTypes if i in itemFilter]\n",
    "elif filterType == '' or itemFilter == []:\n",
    "    pass\n",
    "else:\n",
    "    raise Exception(f'Correct filter type not specified. Exclude, Include, or an empty string are the only accepted inputs. Your input: {filterType}.')\n",
    "\n",
    "#Try regular search\n",
    "##If over 500, search by user\n",
    "##If user has over 500, try to search by item type and user\n",
    "if itemFilter != []:\n",
    "    query = '\" OR \"'.join(itemFilter)\n",
    "    query = f'type: (\"{query}\"), NOT owner: esri*'\n",
    "    itemInfo = gis.content.search(query=query,max_items=500)\n",
    "else:\n",
    "    itemInfo = gis.content.search(query='NOT owner:esri*',max_items=500) \n",
    "\n",
    "if len(itemInfo) == 500:\n",
    "    itemInfo.clear()\n",
    "    users = gis.users.search(max_users = 10000)\n",
    "    users = [user for user in users if user.storageUsage != 0]\n",
    "    for user in users:\n",
    "        if itemFilter != []: \n",
    "            items = gis.content.search(query=f'type: (\"{query}\"), owner: {user.username}', outside_org=False, max_items=500)\n",
    "        else:\n",
    "            items = gis.content.search(query=f'owner: {user.username}', outside_org=False, max_items=500)\n",
    "            \n",
    "        if not items:\n",
    "            pass\n",
    "        elif len(items) < 500:\n",
    "            itemInfo.extend(items)\n",
    "        elif len(items) != 500:\n",
    "            for itemType in itemFilter:\n",
    "                itemsByType = gis.content.search(query=f'type: {itemType} AND owner: {user.username}',  outside_org=False, max_items=500)\n",
    "                if len(itemsByType) == 0:\n",
    "                    pass\n",
    "                if len(itemsByType) == 500:                \n",
    "                    warnings.warn(f'{user.username} has over 500 portal items of a single type. Unable to query all {itemType} items. Items currently queried added to content list.')\n",
    "                    itemInfo.extend(itemsByType)\n",
    "                else:\n",
    "                    itemInfo.extend(itemsByType)\n",
    "        else:\n",
    "            Exception('More than 500 items returned. Please update script with new max_items arguement.')\n",
    "elif len(itemInfo) == 0:\n",
    "    raise Exception('No items returned with the function gis.content.search.')\n",
    "    \n",
    "#Get group data\n",
    "groups = gis.groups.search(max_groups=-1)\n",
    "groupItems = []\n",
    "groupName = []\n",
    "for group in groups:\n",
    "    groupItem = group.content()\n",
    "    groupItems.extend(groupItem)\n",
    "    for item in groupItem:\n",
    "        groupName.append(group.title)\n",
    "\n",
    "groupItems = pd.DataFrame(groupItems)\n",
    "groupItems['group'] = groupName\n",
    "groupItems = groupItems[['id','group']]\n",
    "groupItems = groupItems.groupby(['id'], as_index=False).agg({'group':', '.join})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through itemID with dependent_upon and dependent_to function\n",
    "dependencies = []\n",
    "dependType = []\n",
    "for item in itemInfo:\n",
    "    dependencyUpon = item.dependent_upon()\n",
    "    dependencies.append(dependencyUpon)\n",
    "    dependType.append('Item Depends On')\n",
    "    #---dependencyTo = item.dependent_to()\n",
    "    #---dependency.append(dependencyTo) \n",
    "    #---dependType.append('Item Dependent To')\n",
    "\n",
    "#Convert to dataframes, add dependency direction, copy total to item info\n",
    "itemInfo = pd.DataFrame(itemInfo)\n",
    "dependencies = pd.DataFrame(dependencies)\n",
    "dependencies['dependencyRelate'] = dependType\n",
    "itemInfo['totalDependencies'] = dependencies['total'].loc[dependencies['dependencyRelate'] == 'Item Depends On']\n",
    "itemInfo['totalDepends'] = dependencies['total'].loc[dependencies['dependencyRelate'] == 'Item Dependent To']\n",
    "\n",
    "#Combine dataframes, rename id column\n",
    "dependencies = pd.merge(dependencies, itemInfo['id'], left_index=True, right_index=True)\n",
    "dependencies.rename(columns = {'id':'itemID'}, inplace = True)\n",
    "\n",
    "#Data formatting\n",
    "## Split each dependency into a new column that preserves item ID\n",
    "dependencies = dependencies.explode('list')\n",
    "\n",
    "## Normalize to seperate into diff columns, concat columns for each type\n",
    "dependNorm = pd.json_normalize(dependencies['list']).set_index(dependencies.index)\n",
    "dependNorm = dependNorm.fillna('')\n",
    "dependNorm['dependency'] = dependNorm[dependNorm.columns.difference(['dependencyType'])].astype(str).sum(axis=1)\n",
    "## Merge dataframes to get original ID values\n",
    "dependencies = pd.merge(dependencies, dependNorm, left_index=True, right_index=True)\n",
    "## Remove all empty rows w/ no dependencies\n",
    "try:\n",
    "    dependencies = dependencies[dependencies['dependencyType'] != '']\n",
    "except:\n",
    "    raise Exception('No items with dependencies found using the provided item type filter.')\n",
    "\n",
    "## Group dataframe to remove duplicate dependencies under each itemID\n",
    "dependencies = dependencies.groupby(['itemID','dependency']).first().reset_index()\n",
    "\n",
    "#Apply functions\n",
    "## Fix dependency types with REST urls\n",
    "dependencies['dependencyType'] = dependencies.apply(lambda x: restType(x['dependencyType'], x['dependency']), axis=1)\n",
    "## Generate URLs for dependencies\n",
    "dependencies['url'] = dependencies.apply(lambda x: urlGen(x['dependencyType'], x['dependency'], portal), axis=1)  \n",
    "dependencies['url'].fillna('', inplace=True)\n",
    "## Check for broken depenedency links and item type, apply testLink function\n",
    "linkDict = {}\n",
    "dependencies['linkTest'] = dependencies.apply(lambda x: list(testLink(x['dependency'], x['dependencyType'], x['url'])), axis=1)\n",
    "dependencies[['broken','itemType','itemTitle']] = pd.DataFrame(dependencies['linkTest'].tolist(), index = dependencies.index) #This line not ideal, can't get linkTest() to go into multiple columns instead of one\n",
    "# Drop all columns but necessary ones\n",
    "dependencies = dependencies.loc[:, dependencies.columns.intersection(['itemID','dependencyType','dependency','dependencyRelate','broken','itemType','itemTitle','url'])]\n",
    "\n",
    "#View dataframe\n",
    "#dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format/Clean Up Item Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add url\n",
    "itemInfo['url'] = portal + f'/home/item.html?id=' + itemInfo['id']\n",
    "#Add Groups\n",
    "itemInfo = itemInfo.merge(groupItems, on='id', how='left')\n",
    "\n",
    "#Fix dates, tags, access, & status, replace NA/None values w/ blanks\n",
    "itemInfo['created'] = pd.to_datetime(itemInfo['created'],unit='ms').dt.date\n",
    "itemInfo['modified'] = pd.to_datetime(itemInfo['modified'],unit='ms').dt.date\n",
    "itemInfo['tags'] = itemInfo['tags'].apply(str)\n",
    "itemInfo.replace('[]','', inplace=True)\n",
    "itemInfo['access'].replace(['public','private','shared','org'], ['Public','Private','Shared with Other Users','Shared with Organization'], inplace=True)\n",
    "itemInfo['contentStatus'].replace(['org_authoritative','deprecated'], ['Authoritative','Deprecated'], inplace=True)\n",
    "itemInfo.fillna('', inplace=True)\n",
    "\n",
    "#Remove extra columns (any not listed)\n",
    "itemInfo = itemInfo.loc[:, itemInfo.columns.intersection(['id','owner','created','modified','title','type','snippet','tags','access','numViews','contentStatus','totalDependencies','totalDepends','group','url'])]\n",
    "\n",
    "#Rename columns\n",
    "itemInfo.rename(columns = {'id':'itemID'}, inplace = True)\n",
    "\n",
    "# ***TEMP Line while dependent_to doesn't work to add missing column***\n",
    "itemInfo['totalDepends'] = 0\n",
    "\n",
    "#View dataframe\n",
    "#itemInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generateNodes == True:\n",
    "    #Apply filters\n",
    "    ##Remove serverId items and ArcGIS Utility dependencies, create opposite dataframe\n",
    "    dependencies_filter = dependencies.loc[dependencies['dependency'].str.contains('services2.arcgis|utility.arcgisonline|services.arcgisonline|services6.arcgis')]\n",
    "    dependencies_filter = pd.concat([dependencies_filter, dependencies[dependencies.dependencyType == 'serverId']], ignore_index = True)\n",
    "    dependencies = dependencies[dependencies.dependencyType != 'serverId']\n",
    "    dependencies = dependencies.loc[~dependencies['dependency'].str.contains('services2.arcgis|utility.arcgisonline|services.arcgisonline|services6.arcgis')]\n",
    "\n",
    "    #Construct graph\n",
    "    ## create graph object\n",
    "    G = nx.Graph()\n",
    "    ## add edges from the dataframe to the graph\n",
    "    for index, row in dependencies.iterrows():\n",
    "        G.add_edge(row['itemID'], row['dependency'])\n",
    "    ## get positions and convert to dataframe\n",
    "    nodeXY = nx.spring_layout(G,seed=1234)\n",
    "    nodeXY = pd.DataFrame(nodeXY).T.reset_index()\n",
    "    nodeXY.columns = ['Node', 'X', 'Y']\n",
    "    ## add 1 to all positions to prevent issues w/ crossing dateline\n",
    "    nodeXY[['X','Y']] += 1\n",
    "    \n",
    "    #Add X,Y to point data\n",
    "    ## add positions \n",
    "    itemInfo = itemInfo.merge(nodeXY, left_on='itemID', right_on='Node', how='outer')\n",
    "    itemInfo.fillna('', inplace=True)\n",
    "    ## pull descriptive data about non-organization items to table\n",
    "    itemInfo = itemInfo.merge(dependencies[['dependency','itemType','url','itemTitle']], left_on='Node', right_on='dependency', how='left', suffixes=('','_x'))\n",
    "    ## merge columns based on values\n",
    "    itemInfo['type'] = itemInfo.apply(lambda x: combineColumns(x['itemID'], x['type'], x['itemType']), axis=1)\n",
    "    itemInfo['url'] = itemInfo.apply(lambda x: combineColumns(x['itemID'], x['url'], x['url_x']), axis=1)\n",
    "    itemInfo['title'] = itemInfo.apply(lambda x: combineColumns(x['itemID'], x['title'], x['itemTitle']), axis=1)\n",
    "    itemInfo.drop(columns={'dependency','itemType','url_x','itemTitle'}, inplace=True)\n",
    "    itemInfo['Node'] = itemInfo.apply(lambda x: combineColumns(x['itemID'], x['itemID'], x['Node']), axis=1)\n",
    "    itemInfo = itemInfo.groupby(['Node']).first().reset_index()\n",
    "    \n",
    "    #Add X,Y to line data\n",
    "    ## original item column (itemID)\n",
    "    dependencies = dependencies.merge(nodeXY, left_on='itemID', right_on='Node', how='left')\n",
    "    dependencies.rename(columns = {'X':'X1','Y':'Y1'}, inplace = True)\n",
    "    ## dependency column (dependency)\n",
    "    dependencies = dependencies.merge(nodeXY, left_on='dependency', right_on='Node', how='left')\n",
    "    dependencies.rename(columns = {'X':'X2','Y':'Y2'}, inplace = True)\n",
    "    ## drop extras created from merge\n",
    "    dependencies.drop(columns=['Node_x','Node_y'], inplace=True)\n",
    "    ##Merge filter dataframe back in\n",
    "    dependencies = pd.concat([dependencies, dependencies_filter], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to CSV\n",
    "if exportType == 'CSV' or 'All':\n",
    "    if outPath == '':\n",
    "        if '\\\\' in workspace:\n",
    "            parentFolder = workspace.split('\\\\')\n",
    "            parentFolder.remove(parentFolder[-1])\n",
    "            parentFolder = '\\\\'.join(parentFolder)\n",
    "        elif '/' in workspace:\n",
    "            parentFolder = workspace.split('/')\n",
    "            parentFolder.remove(parentFolder[-1])\n",
    "            parentFolder = '/'.join(parentFolder)\n",
    "        elif workspace == 'Memory' or 'memory':\n",
    "            warnings.warn('Cannot export CSVs to memory.')\n",
    "        else:\n",
    "            raise Exception('Geodatabase path is not valid.')\n",
    "\n",
    "        dependencies.to_csv(f'{parentFolder}/{lineName}.csv',index=False)\n",
    "        itemInfo.to_csv(f'{parentFolder}/{pointName}.csv',index=False)\n",
    "    else:\n",
    "        dependencies.to_csv(f'{outPath}/{lineName}.csv',index=False)\n",
    "        itemInfo.to_csv(f'{outPath}/{pointName}.csv',index=False)\n",
    "elif exportType == 'Features' or exportType == '':\n",
    "    pass\n",
    "else:\n",
    "    warnings.warn('No acceptable export type specified. Data has not been exported as CSV files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data for ArcPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exportType == ('Features' or 'Geo Table' or 'All'):\n",
    "    itemInfo = itemInfo.replace({np.nan: None,'': None})\n",
    "    itemInfo.replace('', 'None', inplace=True)\n",
    "    itemInfo['numViews'].fillna(0, inplace=True)\n",
    "    itemInfo['totalDependencies'].fillna(0, inplace=True)\n",
    "    itemInfo['totalDepends'].fillna(0, inplace=True)\n",
    "    itemInfo = itemInfo.astype({'itemID':'string','owner':'string','created':'string','modified':'string','title':'string',\n",
    "                              'type':'string','snippet':'string','tags':'string','url':'string',\n",
    "                              'access':'string','numViews':'int','contentStatus':'string','group':'string',\n",
    "                              'totalDependencies':'int','totalDepends':'int','Node':'string'})\n",
    "    \n",
    "    dependencies = dependencies.replace({np.nan: None,'':None})\n",
    "    dependencies = dependencies.astype({'itemID':'string','dependency':'string','dependencyRelate':'string','dependencyType':'string',\n",
    "                            'url':'string','broken':'string','itemType':'string','itemTitle':'string'})\n",
    "\n",
    "#Change Dtypes for X,Y\n",
    "if generateNodes == True:\n",
    "    itemInfo = itemInfo.astype({'X':'float','Y':'float'})\n",
    "    dependencies = dependencies.astype({'X1':'float','Y1':'float','X2':'float','Y2':'float'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to feature layers\n",
    "if exportType == ('Features' or 'All') and generateNodes == True:\n",
    "    ## use arcpy to create feature layers\n",
    "    if workspace == ('Memory' or 'memory'):\n",
    "        pointLayer = arcpy.management.XYTableToPoint(in_table = itemInfo, out_feature_class = f'memory/{pointName}', x_field = 'X', y_field = 'Y', coordinate_system = arcpy.SpatialReference(4326))\n",
    "        lineLayer = arcpy.management.XYToLine(dependencies, f'memory/{lineName}','X1','Y1','X2','Y2', arcpy.SpatialReference(4326), attributes = 'ATTRIBUTES')\n",
    "    else:\n",
    "        pointLayer = arcpy.management.XYTableToPoint(in_table = itemInfo, out_feature_class = f'{workspace}/{pointName}', x_field = 'X', y_field = 'Y', coordinate_system = arcpy.SpatialReference(4326))\n",
    "        lineLayer = arcpy.management.XYToLine(dependencies, f'{workspace}/{lineName}','X1','Y1','X2','Y2', arcpy.SpatialReference(4326), attributes = 'ATTRIBUTES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to GDB Tables (Non-Spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exportType == ('Geo Table' or 'All'):\n",
    "        if workspace == ('Memory' or 'memory'):\n",
    "            pointTable = arcpy.conversion.ExportTable(in_table = itemInfo, out_table= f'memory/{pointName}', use_field_alias_as_name='NOT_USE_ALIAS') #field_mapping = {}\n",
    "            lineTable = arcpy.conversion.ExportTable(in_table = dependencies, out_table= f'memory/{lineName}', use_field_alias_as_name='NOT_USE_ALIAS') #field_mapping = {}\n",
    "        else:\n",
    "            pointTable = arcpy.conversion.ExportTable(in_table = itemInfo, out_table= f'{workspace}/{pointName}', use_field_alias_as_name='NOT_USE_ALIAS') #field_mapping = {}\n",
    "            lineTable = arcpy.conversion.ExportTable(in_table = dependencies, out_table= f'{workspace}/{lineName}', use_field_alias_as_name='NOT_USE_ALIAS') #field_mapping = {} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
